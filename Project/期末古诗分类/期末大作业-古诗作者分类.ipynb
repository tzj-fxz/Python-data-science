{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc59bb75-c8e8-4784-b8a3-21eb1c6aa1ca",
   "metadata": {},
   "source": [
    "# RNN序列编码-分类期末大作业\n",
    "\n",
    "本次大作业要求手动实现双向LSTM+基于attention的聚合模型，并用于古诗作者预测的序列分类任务。**请先阅读ppt中的作业说明。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9638967f-db5b-425e-9c5b-167b2d871165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0fee5-d222-4d07-8d6c-a5a1a97ac1b2",
   "metadata": {},
   "source": [
    "## 1. 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c3e8f-d806-4137-997b-1b3018839f4a",
   "metadata": {},
   "source": [
    "数据位于`data`文件夹中，每一行对应一个样例，格式为“诗句 作者”。下面的代码将数据文件读取到`train_data`, `valid_data`和`test_data`中，并根据训练集中的数据构造词表`word2idx`/`idx2word`和标签集合`label2idx`/`idx2label`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "408f1a0a-afb3-4c8c-968e-2a037bc6c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {\"<unk>\": 0}\n",
    "label2idx = {}\n",
    "idx2word = [\"<unk>\"]\n",
    "idx2label = []\n",
    "\n",
    "train_data = []\n",
    "with open(\"data/train.txt\") as f:\n",
    "    for line in f:\n",
    "        text, author = line.strip().split()\n",
    "        for c in text:\n",
    "            if c not in word2idx:\n",
    "                word2idx[c] = len(idx2word)\n",
    "                idx2word.append(c)\n",
    "        if author not in label2idx:\n",
    "            label2idx[author] = len(idx2label)\n",
    "            idx2label.append(author)\n",
    "        train_data.append((text, author))\n",
    "\n",
    "valid_data = []\n",
    "with open(\"data/valid.txt\") as f:\n",
    "    for line in f:\n",
    "        text, author = line.strip().split()\n",
    "        valid_data.append((text, author))\n",
    "\n",
    "test_data = []\n",
    "with open(\"data/test.txt\") as f:\n",
    "    for line in f:\n",
    "        text, author = line.strip().split()\n",
    "        test_data.append((text, author))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1977f00-e7ad-43e8-8a5a-689fdb515c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4941 4941 5 5\n",
      "11271 1408 1410\n"
     ]
    }
   ],
   "source": [
    "print(len(word2idx), len(idx2word), len(label2idx), len(idx2label))\n",
    "print(len(train_data), len(valid_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a1eb4-e262-4bf8-9fd4-4ab52eebf476",
   "metadata": {},
   "source": [
    "**请完成下面的函数，其功能为给定一句古诗和一个作者，构造RNN的输入。** 这里需要用到上面构造的词表和标签集合，对于不在词表中的字用\\<unk\\>代替。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e13c2f-064b-42a0-8f00-d39da3fff1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(text, author):\n",
    "    \"\"\"\n",
    "    输入\n",
    "        text: str\n",
    "        author: str\n",
    "    输出\n",
    "        x: LongTensor, shape = (1, text_length)\n",
    "        y: LongTensor, shape = (1,)\n",
    "    \"\"\"\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2ee07-19c9-4568-aaff-019fe46a4bd3",
   "metadata": {},
   "source": [
    "## 2. LSTM算子（单个时间片作为输入）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a5c0b-9d6d-4166-ae52-a68f7490ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.g = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, ht, ct, xt):\n",
    "        # ht: 1 * hidden_size\n",
    "        # ct: 1 * hidden_size\n",
    "        # xt: 1 * input_size\n",
    "        input_combined = torch.cat((xt, ht), 1)\n",
    "        ft = torch.sigmoid(self.f(input_combined))\n",
    "        it = torch.sigmoid(self.i(input_combined))\n",
    "        ot = torch.sigmoid(self.o(input_combined))\n",
    "        gt = torch.tanh(self.g(input_combined))\n",
    "        ct = ft * ct + it * gt\n",
    "        ht = ot * torch.tanh(ct)\n",
    "        return ht, ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc96e0-1f06-43a7-aeb8-08e837c2eede",
   "metadata": {},
   "source": [
    "## 3. 实现双向LSTM（整个序列作为输入）\n",
    "\n",
    "**要求使用上面提供的LSTM算子，不要调用torch.nn.LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27bc3e-5c90-4af2-b842-9eb4d5fc0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        # TODO\n",
    "        raise NotImplementedError\n",
    "        self.register_buffer(\"_float\", torch.zeros(1, hidden_size))\n",
    "    \n",
    "    def init_h_and_c(self):\n",
    "        h = torch.zeros_like(self._float)\n",
    "        c = torch.zeros_like(self._float)\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        输入\n",
    "            x: 1 * length * input_size\n",
    "        输出\n",
    "            hiddens\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        raise NotImplementedError\n",
    "        return hiddens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc8fca-caff-4a6d-abf1-0ced79692cc2",
   "metadata": {},
   "source": [
    "## 4. 实现基于attention的聚合机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0122d8c3-ba8e-4b02-a9ae-8e06102ddcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        # TODO\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, hiddens):\n",
    "        \"\"\"\n",
    "        输入\n",
    "            hiddens: 1 * length * hidden_size\n",
    "        输出\n",
    "            attn_outputs: 1 * hidden_size\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        raise NotImplementedError\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62bbf4-b565-4ec2-aebb-1d5fcbb7e048",
   "metadata": {},
   "source": [
    "## 5. 利用上述模块搭建序列分类模型\n",
    "\n",
    "参考模型结构：Embedding – BiLSTM – Attention – Linear – LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90edb70-7c1d-4839-9f0b-c8e8dbf9a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, num_vocab, embedding_dim, hidden_size, num_classes):\n",
    "        \"\"\"\n",
    "        参数\n",
    "            num_vocab: 词表大小\n",
    "            embedding_dim: 词向量维数\n",
    "            hidden_size: 隐状态维数\n",
    "            num_classes: 类别数量\n",
    "        \"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # TODO\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        输入\n",
    "            x: 1 * length, LongTensor\n",
    "        输出\n",
    "            outputs\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        raise NotImplementedError\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0325df21-7e77-4883-8909-9352e90a37fe",
   "metadata": {},
   "source": [
    "## 6. 请利用上述模型在古诗作者分类任务上进行训练和测试\n",
    "\n",
    "要求选取在验证集上效果最好的模型，输出测试集上的准确率、confusion matrix以及macro-precision/recall/F1，并打印部分测试样例及预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56a57606-cbbd-4532-bbac-8cb974fac21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e34f22132bbfe8770606924452c45dc19194fd90a2d63a36f15be83095f1b0a6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorchcuda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas数据预处理与PyTorch的数据加载\n",
    "\n",
    "## 一、Pandas简单数据预处理\n",
    "Pandas是python的一个数据分析包，提供了大量能使我们快速便捷地处理数据的函数和方法。\n",
    "本文以一个简单的样例介绍如何使用pandas处理原始数据并将它们转化为tensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先创建一个csv文件作为原始数据，将数据按行写入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_file = 'house_tiny.csv'\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
    "    f.write('NA,Pave,127500\\n')  # Each row represents a data example\n",
    "    f.write('NA,Pave,127500\\n')\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来使用pandas包中的``read_csv()``方法读取csv文件，读入的数据以DataFrame的形式呈现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       NaN  Pave  127500\n",
      "2       2.0   NaN  106000\n",
      "3       4.0   NaN  178100\n",
      "4       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看数据相关属性:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   NumRooms  2 non-null      float64\n",
      " 1   Alley     2 non-null      object \n",
      " 2   Price     5 non-null      int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 248.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>135820.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.414214</td>\n",
       "      <td>26611.783104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>106000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>127500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>127500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.500000</td>\n",
       "      <td>140000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>178100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       NumRooms          Price\n",
       "count  2.000000       5.000000\n",
       "mean   3.000000  135820.000000\n",
       "std    1.414214   26611.783104\n",
       "min    2.000000  106000.000000\n",
       "25%    2.500000  127500.000000\n",
       "50%    3.000000  127500.000000\n",
       "75%    3.500000  140000.000000\n",
       "max    4.000000  178100.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # 查看行数与列数\n",
    "data.head() # 查看数据前5行\n",
    "data.info() # 查看每一列的计数及数据类型等信息\n",
    "data.describe() # 查看统计信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用``duplicated()``和``drop_duplicates()``方法可以实现数据去重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看是否有重复项\n",
    "'''\n",
    "参数keep影响返回的Boolean值。\n",
    "\"first\": 重复项中第一项设置为False，其他为True\n",
    "\"last\": 最后一项为False，其他为True\n",
    "False: 所有重复项都为True\n",
    "'''\n",
    "data.duplicated(keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "2       2.0   NaN  106000\n",
      "3       4.0   NaN  178100\n",
      "4       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "# 去除重复项\n",
    "'''\n",
    "参数keep的效果与duplicated()相对应，将会删除被设置为True的重复项\n",
    "'''\n",
    "data = data.drop_duplicates(keep=\"first\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用iloc可以根据索引获得数据切片，这里将data的前两列划为inputs，最后一列化为outputs："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley\n",
      "0       NaN  Pave\n",
      "2       2.0   NaN\n",
      "3       4.0   NaN\n",
      "4       NaN   NaN\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据中存在多个NaN，常用处理缺失数据的方法可以分为两种：\n",
    "- imputation：用其他值替换缺失值\n",
    "- deletion：忽略缺失值\n",
    "\n",
    "对于NumRooms这一列，调用``fillna()``方法，用该列其他数据的平均值补充："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley\n",
      "0       3.0  Pave\n",
      "2       2.0   NaN\n",
      "3       4.0   NaN\n",
      "4       3.0   NaN\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于Alley这一列，使用``get_dummies()``方法，它将``Pave``与``NaN``看作是不同的类别而拆分成两列，结果可以看成是一种one-hot编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0       3.0           1          0\n",
      "2       2.0           0          1\n",
      "3       4.0           0          1\n",
      "4       3.0           0          1\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical variable into dummy/indicator variables\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs与outputs中的每一项都是数字，借助numpy可以转化成tensor格式，之后就能方便地进行各种张量运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3., 1., 0.],\n",
       "        [2., 0., 1.],\n",
       "        [4., 0., 1.],\n",
       "        [3., 0., 1.]]),\n",
       " array([127500, 106000, 178100, 140000], dtype=int64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X, y = np.array(inputs.values), np.array(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上只涵盖了pandas使用方法的极小部分内容，关于pandas更详细的使用说明可以阅读官方文档：https://pandas.pydata.org/docs/user_guide/index.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、PyTorch数据加载\n",
    "PyTorch中关于数据集的相关处理主要涉及两个类，``torch.utils.data.Dataset``和``torch.utils.data.DataLoader``。本文对此进行简单的介绍，更多详细内容可以阅读``torch.utils.data``的官方文档：\n",
    "- 英文：https://pytorch.org/docs/stable/data.html\n",
    "- 中文：https://pytorch-cn.readthedocs.io/zh/latest/package_references/data/\n",
    "\n",
    "### Dataset类\n",
    "\n",
    "Dataset是一个抽象类，在使用PyTorch时，为了方便读取，我们需要将使用的数据包装为Dataset类。自定义的数据集应该作为它的子类，并且重写``Dataset``的两个成员函数：``__getitem__``与``__len__``。\n",
    "- ``__getitem__``：必须实现，返回指定key所对应的数据样本。\n",
    "- ``__len__``：可选，返回数据集的大小。\n",
    "\n",
    "### Dataloader类\n",
    "\n",
    "Dataloader类将数据集与采样器结合，返回给定数据集上的一个迭代器。\n",
    "相比起用简单的for循环来迭代数据，dataloader保留了更多的信息，它使得我们可以按批读取数据、打乱数据顺序、使用多进程并行加载数据等等。\n",
    "\n",
    "Dataloder完整的参数如下：\n",
    "```python\n",
    "class torch.utils.data.DataLoader(\n",
    "    dataset: torch.utils.data.dataset.Dataset[T_co],  # 传入的数据集\n",
    "    batch_size: Optional[int] = 1,              # 每个batch的样本数\n",
    "    shuffle: bool = False,                   # 在每个epoch开始前是否对数据进行重新排序\n",
    "    sampler: Optional[torch.utils.data.sampler.Sampler[int]] = None,  # 定义从数据集中取样的策略\n",
    "    batch_sampler: Optional[torch.utils.data.sampler.Sampler[Sequence[int]]] = None,  # 与sampler类似，每次返回一个batch的索引。与batch_size，shuffle，sampler和drop_last互斥\n",
    "    num_workers: int = 0,                # 加载数据时使用的子进程数，0为不使用多进程\n",
    "    collate_fn: Callable[List[T], Any] = None,  # 将一个list的样本组合成一个mini-batch\n",
    "    pin_memory: bool = False,  # 是否在返回前将tensors拷贝到CUDA的固定内存中\n",
    "    drop_last: bool = False,   # 数据集大小不是batch_size的整数倍时，是否丢弃最后一个不完整的batch\n",
    "    \n",
    "    # 以下是使用多进程加载数据时，关于各个worker的参数设置\n",
    "    timeout: float = 0, \n",
    "    worker_init_fn: Callable[int, None] = None, \n",
    "    multiprocessing_context=None, generator=None,\n",
    "    prefetch_factor: int = 2, \n",
    "    persistent_workers: bool = False\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch数据加载可以分为两种情况：\n",
    "- 加载PyTorch自带数据集\n",
    "- 加载自己的数据集\n",
    "\n",
    "### 加载PyTorch自带数据集\n",
    "在``torchvision.datasets``中，包括了MNIST，Imagenet-12，CIFAR等常用数据集，且所有数据集都是``torch.utils.data.Dataset``的子类。\n",
    "\n",
    "参考文档：https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "以手写数字MNIST数据集为例，运行以下代码，将会完成数据集的导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "MNIST_set = torchvision.datasets.MNIST(\n",
    "    root = 'MNIST/MNIST/', # 数据集的路径（MNIST所在的文件夹）\n",
    "    train = True,   # 从训练集training.pt或测试集test.pt创建数据集\n",
    "    download = False # True表示从网上下载并放入指定目录，如果已经自己下载了，设置为False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试着查看数据，``MNIST_set[0]``包含了一张图片——手写数字5，以及它的标签“5”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x19459B2B970>, 5)\n",
      "<PIL.Image.Image image mode=L size=28x28 at 0x19459B2B400>\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(MNIST_set[0])\n",
    "img, label = MNIST_set[0]\n",
    "print(img)\n",
    "print(label)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来进行Dataloader的实例化。\n",
    "由于数据中包含了图像，首先需要将他们转换为tensor，否则在之后会报错：``batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>``\n",
    "\n",
    "可以使用torchvision.transform模块进行转换，该模块提供了一般的图形转换操作类，用于数据处理，通常的预处理操作有：改变图像尺寸，数据增强（随即切片，镜像），数据维度变换等等。\n",
    "感兴趣的同学可以查阅相关文档：https://pytorch.org/docs/stable/torchvision/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "mytransform = transforms.Compose([\n",
    "    transforms.ToTensor() # 将PIL Image或numpy.ndarray转换为tensor\n",
    "    # 还可以增加别的变换\n",
    "])\n",
    "MNIST_set = torchvision.datasets.MNIST(\n",
    "    root = './MNIST',\n",
    "    train = True,\n",
    "    download = False,\n",
    "    transform = mytransform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = MNIST_set[0]\n",
    "print(img)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就可以进行dataloader的构造："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_loader = torch.utils.data.DataLoader(\n",
    "    MNIST_set,\n",
    "    batch_size = 10,\n",
    "    shuffle = False,\n",
    "    num_workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示部分数据，简单测试是否读取成功："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data, label) in enumerate(MNIST_loader):\n",
    "    img = transforms.ToPILImage()(data[0][0]) # 将tensor转换为图片\n",
    "    print(label[0]) # 图片对应的数字标签\n",
    "    img.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造完的dataloader会包含两部分数据：数据集中的图像构成的tensor，图像对应的标签构成的tensor。\n",
    "dataloader是一个可迭代对象，除了通过for循环访问数据，也可以使用迭代器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = iter(MNIST_loader)\n",
    "batch_data = next(data_iterator)\n",
    "print(batch_data[0]) # image tensor\n",
    "print(batch_data[1]) # label tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载自己的数据集\n",
    "\n",
    "加载自己的数据集方式也由这两个步骤构成：先创建一个自定义的数据集类继承Dataset，再使用Dataloader读取数据。\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 定义自己的数据集，继承Dataset类\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # 做一些初始化处理，例如数据集路径，文件名列表等\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 从文件中读取一条数据，做一些处理并返回\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 返回数据集的大小\n",
    "        pass\n",
    "    \n",
    "mydataset = Mydataset(xxx)\n",
    "mydataloader = torch.utils.data.DataLoader(mydataset, batch_size = 10, shuffle = True, num_workers = 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们来看看PyTorch教程中的一个样例，需要预先安装scikit-image与pandas。\n",
    "\n",
    "原文地址：https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "首先从这里下载所需的数据集：https://download.pytorch.org/tutorial/faces.zip\n",
    "该数据集的内容是关于面部姿势，对每张人脸图像都用了68个标记点进行标注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入接下来需要用到的库\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# 忽略warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # 开启交互模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下载好的faces文件夹中，有许多人脸的jpg文件，以及含有标记数据的csv文件，它包含了每张图片的文件名以及它对应的各个标记点的x轴y轴坐标，内容格式像这样：\n",
    "```\n",
    "image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y\n",
    "0805personali01.jpg,27,83,27,98, ... 84,134\n",
    "1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312\n",
    "```\n",
    "使用pandas读取csv文件，获取标记点的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_frame = pd.read_csv('faces/face_landmarks.csv')\n",
    "\n",
    "n = 65 # 先随意取出一个样本看看\n",
    "img_name = landmarks_frame.iloc[n, 0]\n",
    "landmarks = landmarks_frame.iloc[n, 1:]\n",
    "landmarks = np.asarray(landmarks)\n",
    "# 以(N, 2)的数组形式获得标记点，其中N表示标记点的个数。\n",
    "landmarks = landmarks.astype('float').reshape(-1, 2) # 使用-1表示该维度的长度会被自动计算\n",
    "\n",
    "print('Image name: {}'.format(img_name))\n",
    "print('Landmarks shape: {}'.format(landmarks.shape))\n",
    "print('First 4 Landmarks: {}'.format(landmarks[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一个简单的帮助函数来显示图像和标记点，并用它来显示上面获取的那个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_landmarks(image, landmarks):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r') # 绘制标记点\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "plt.figure()\n",
    "show_landmarks(io.imread(os.path.join('faces/', img_name)), landmarks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来构造我们自定义的数据集类。\n",
    "\n",
    "在``__init__``中读取csv文件，在``__getitem__``中读取图片。在``__getitem__``中读取图片的好处是这样图片不需要一下子全部读进内存，而是每次读取一批，可以充分利用内存空间。\n",
    "\n",
    "我们的数据集以字典的形式存储：``{'image': image, 'landmarks': landmarks}``。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): 标记点csv文件的路径\n",
    "            root_dir (string): 存放所有图片的路径\n",
    "            transform (callable, optional): 可选的transform操作\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        # 根据索引从csv文件获取图片文件名\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        # 构造字典\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来进行该数据集类的实例化，并查看前四个样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n",
    "                                    root_dir='faces/')\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(face_dataset)):\n",
    "    sample = face_dataset[i]\n",
    "\n",
    "    print(i, sample['image'].shape, sample['landmarks'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    show_landmarks(**sample)\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面展示的四个样本可以看出，这些样本的大小各不相同，大部分的神经网络都要求图片以固定的尺寸输入网络，我们最好先进行一些预处理。创建三种变换：\n",
    "- ``Rescale``：修改图片尺寸\n",
    "- ``RandomCrop``：随机裁剪图片，是一种数据增强方法\n",
    "- ``ToTensor``：从numpy images转换为torch images\n",
    "\n",
    "这里将它们写成可调用的类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"将样本中图片修改为规定的尺寸.\n",
    "\n",
    "    参数:\n",
    "        output_size (int 或者 tuple): 要求的输出尺寸. 如果是tuple, 输出和output_size匹配。\n",
    "如果是int, 图片的短边和output_size匹配，长边按比例缩放。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        # 缩放处理\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # 对landmarks来说h和w需要交换位置，因为对图片来说，x和y分别是第1维和第0维\n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"随机裁剪图片.\n",
    "\n",
    "    参数:\n",
    "        output_size (tuple or int): 期望的输出尺寸. 如果是int, 做正方形裁剪.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        landmarks = landmarks - [left, top]\n",
    "\n",
    "        return {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"将ndarrays转化为Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        # 交换颜色通道，因为\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'landmarks': torch.from_numpy(landmarks)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们就可以实例化一个经过预处理的数据集，所有样本的大小都得到统一："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n",
    "                                           root_dir='faces/',\n",
    "                                           transform=transforms.Compose([\n",
    "                                               Rescale(256),\n",
    "                                               RandomCrop(224),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "for i in range(len(transformed_dataset)):\n",
    "    sample = transformed_dataset[i]\n",
    "\n",
    "    print(i, sample['image'].size(), sample['landmarks'].size())\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以使用dataloader进行数据迭代了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "# 显示一批的数据\n",
    "def show_landmarks_batch(sample_batched):\n",
    "    \"\"\"显示一批样本的图片和标记点.\"\"\"\n",
    "    images_batch, landmarks_batch = \\\n",
    "            sample_batched['image'], sample_batched['landmarks']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size,\n",
    "                    landmarks_batch[i, :, 1].numpy(),\n",
    "                    s=10, marker='.', c='r')\n",
    "\n",
    "        plt.title('Batch from dataloader')\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['landmarks'].size())\n",
    "\n",
    "    # 观察到第4批的时候就停止.\n",
    "    if i_batch == 3:\n",
    "        plt.figure()\n",
    "        show_landmarks_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff() # 1\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "以上内容就是本次对pandas数据预处理以及PyTorch数据加载的简单介绍，感谢大家的阅读。\n",
    "有内容上的错误或疑问可以联系作者进行讨论：1700012810@pku.edu.cn。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79519f96375e4fd383fdfe7f241b08e17aafdbbba98d62a2cd6b64f01699b8f4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('mypytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

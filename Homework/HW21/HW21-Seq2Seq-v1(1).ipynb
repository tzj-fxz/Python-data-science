{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ed6129-caa4-4d67-83f7-b8fcd0e465e6",
   "metadata": {},
   "source": [
    "# Seq2Seq作业\n",
    "\n",
    "本次作业的目的是使用Seq2Seq模型进行法语-英语机器翻译任务。请先使用`pip install sacrebleu`安装sacrebleu库用于计算BLEU评测指标。\n",
    "\n",
    "本次作业侧重于对代码的理解和对实验结果的分析，模型表现不会作为评分依据。\n",
    "\n",
    "**截止时间：6月5日**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c77a1-4912-428b-a15a-8da2ee4cae14",
   "metadata": {},
   "source": [
    "## 1. 读取数据\n",
    "\n",
    "本次作业使用的数据位于`fr-en.txt`文件中，每一行是一组数据，形式为“法语句子\\t英语句子”。其中的法语句子和英语句子均已经过预处理，可以直接按照空格切分为单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c078f2b2-485c-4c84-a58e-a3ee8dd84c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sacrebleu.metrics import BLEU\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "568a60a2-add6-4d8c-b04c-1408da8ccb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_sents = []\n",
    "en_sents = []\n",
    "with open(\"fr-en.txt\") as f:\n",
    "    for line in f:\n",
    "        fr, en = line.strip().split(\"\\t\")\n",
    "        fr = fr.split()\n",
    "        en = en.split()\n",
    "        fr_sents.append(fr)\n",
    "        en_sents.append(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c1874-8e1f-49c4-8fb7-f44758a2093c",
   "metadata": {},
   "source": [
    "将数据打乱后按照8:1:1切分为训练集、验证集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab828924-7665-4d0f-b435-d4b00f4e1434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 验证集 测试集大小分别为 16000 2000 2000\n"
     ]
    }
   ],
   "source": [
    "idx = list(range(len(fr_sents)))\n",
    "random.shuffle(idx)\n",
    "_fr_sents = [fr_sents[_] for _ in idx]\n",
    "_en_sents = [en_sents[_] for _ in idx]\n",
    "\n",
    "N = len(fr_sents)\n",
    "N_train = int(N * 0.8)\n",
    "N_valid = int(N * 0.1)\n",
    "N_test = N - N_train - N_valid\n",
    "\n",
    "train_fr = _fr_sents[:N_train]\n",
    "train_en = _en_sents[:N_train]\n",
    "valid_fr = _fr_sents[N_train:N_train+N_valid]\n",
    "valid_en = _en_sents[N_train:N_train+N_valid]\n",
    "test_fr = _fr_sents[N_train+N_valid:]\n",
    "test_en = _en_sents[N_train+N_valid:]\n",
    "\n",
    "print(\"训练集 验证集 测试集大小分别为\", N_train, N_valid, N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b4b3a-c4d2-4c7e-856a-6810e652f047",
   "metadata": {},
   "source": [
    "定义词表类Vocab，用于记录两种语言中出现的单词及其编号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "183276b1-bbf1-402c-b020-7c4a29778571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.word2cnt = {}\n",
    "        self.idx2word = []\n",
    "        self.add_word(\"[BOS]\")\n",
    "        self.add_word(\"[EOS]\")\n",
    "        self.add_word(\"[UNK]\")\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        将单词word加入到词表中\n",
    "        \"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.word2cnt[word] = 0\n",
    "            self.word2idx[word] = len(self.idx2word)\n",
    "            self.idx2word.append(word)\n",
    "        self.word2cnt[word] += 1\n",
    "    \n",
    "    def add_sent(self, sent):\n",
    "        \"\"\"\n",
    "        将句子sent中的每一个单词加入到词表中\n",
    "        sent是由单词构成的list\n",
    "        \"\"\"\n",
    "        for word in sent:\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def index(self, word):\n",
    "        \"\"\"\n",
    "        若word在词表中则返回其下标，否则返回[UNK]对应序号\n",
    "        \"\"\"\n",
    "        return self.word2idx.get(word, self.word2idx[\"[UNK]\"])\n",
    "    \n",
    "    def encode(self, sent):\n",
    "        \"\"\"\n",
    "        在句子sent的首尾分别添加BOS和EOS之后编码为整数序列\n",
    "        \"\"\"\n",
    "        encoded = [self.word2idx[\"[BOS]\"]] + [self.index(word) for word in sent] + [self.word2idx[\"[EOS]\"]]\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, encoded, strip_bos_and_eos=False):\n",
    "        \"\"\"\n",
    "        将整数序列解码为单词序列\n",
    "        \"\"\"\n",
    "        return [self.idx2word[_] for _ in encoded if not strip_bos_and_eos or self.idx2word[_] not in [\"[BOS]\", \"[EOS]\"]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回词表大小\n",
    "        \"\"\"\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19127dbb-f224-428b-8f9c-6970363757ea",
   "metadata": {},
   "source": [
    "对于两种语言分别构建词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38e6c29f-9201-4d6a-9b85-5b37bd5014e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "法语词表大小为 3152\n",
      "英语词表大小为 5463\n"
     ]
    }
   ],
   "source": [
    "fr_vocab = Vocab()\n",
    "en_vocab = Vocab()\n",
    "\n",
    "for fr, en in zip(train_fr, train_en):\n",
    "    fr_vocab.add_sent(fr)\n",
    "    en_vocab.add_sent(en)\n",
    "\n",
    "print(\"法语词表大小为\", len(en_vocab))\n",
    "print(\"英语词表大小为\", len(fr_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa0cf12-3e2d-4008-81a4-a884ba6303ca",
   "metadata": {},
   "source": [
    "## 2. Seq2Seq模型\n",
    "\n",
    "### 1) Attention\n",
    "机器翻译中，目标语言中的每个单词往往都对应于源语言中的一个或几个单词，在翻译时，如果模型能够学习到这种对应关系，则可以一定程度上缓解RNN中的长程信息损失问题，对翻译效果带来很大提升。\n",
    "\n",
    "Attention机制正是基于这样的思想，可以在解码过程中关注到编码器端不同时间片的信息。在解码过程中的每个时间片，attention会利用当前解码端的隐状态，对编码端的每个时间步计算一个相关性分数。这个相关性分数使用Softmax进行归一化后，作为权重对编码端的隐状态序列加权求和，即得到解码端当前时间片的注意力向量。这个注意力向量中包含了源语言中和当前时间片最相关的信息，可以用于当前时间片的输出预测及隐状态更新。\n",
    "\n",
    "记当前解码端隐状态为$\\mathbf{h_t}$, 编码器端的隐状态为$\\mathbf{s_j}, j=0 \\ldots L-1$, 一些常见的相关性分数计算方法为：\n",
    "- $\\alpha_{t,j}=\\mathbf{h_t}^T \\mathbf{s_j}$\n",
    "- $\\alpha_{t,j}=\\mathbf{h_t}^T W \\mathbf{s_j}$\n",
    "- $\\alpha_{t,j}=\\mathbf{v}^T \\mathrm{tanh}(W_1 \\mathbf{h_t} + W_2 \\mathbf{s_j})$\n",
    "\n",
    "其中$W, W_1, W_2, \\mathbf{v}$都为神经网络的可学习参数。解码端该时间片的注意力向量为 $\\mathbf{c_t} = \\sum_{j=0}^{L-1} \\alpha_{t,j}\\mathbf{s_j}$。\n",
    "\n",
    "### 2) 本次作业中使用的Seq2Seq模型（也可直接阅读代码）\n",
    "本次作业使用的Seq2Seq模型如下（其中N为batch size，H为hidden size）：\n",
    "- 注意力模块Attention\n",
    "    - 输入包括解码器端的隐状态`decoder_hidden_state`（大小为N \\* H）和编码器端的隐状态序列`encoder_hidden_states`（大小为N \\* L \\* H，其中L为源语言长度）\n",
    "    - 对于batch中的第i个样例，Attention的输出向量是对编码器端隐状态序列的加权求和\n",
    "$$ \\mathrm{attn\\_output}_i = \\sum_{t=0}^{L-1} \\alpha_{i,t} ~\\mathrm{Enc_{i, t}} $$\n",
    "    其中权重采取1)中的第二种计算方法，即$\\alpha_{i,t} = \\mathrm{Dec}_{i}^T ~ W ~ \\mathrm{Enc}_{i,t}$，其中$W$是个可学习的线性变换。\n",
    "    - 输出的`attn_output`大小为N * H。\n",
    "- 编码器EncoderRNN\n",
    "    - 输入包括源语言单词下标序列`input`（大小为N）以及RNN初始隐状态`hidden`（大小为N * H，初始化为0向量）；\n",
    "    - 本次作业使用[nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)对词向量进行学习，词向量的维数设为`embedding_dim`;\n",
    "    - 本次作业使用pytorch提供的[nn.GRUCell](https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html);\n",
    "    - 在每个时间片，使用Embedding将单词的下标映射为对应的词向量（大小为N \\* embedding_dim），作为RNN的输入向量，连同上一个时间片的`hidden`一起输入到GRUCell中，得到当前时间片的`hidden`；\n",
    "    - 由于在编码器端不做预测，因此不需要得到output；\n",
    "    - 输出当前时间片的隐状态`hidden`，大小为N \\* H。\n",
    "- 解码器DecoderRNN\n",
    "    - 输入包括目标语言单词下标序列`input`（大小为N）、RNN初始隐状态`hidden`（大小为N \\* H，初始化为编码器的最终隐状态）以及编码器端隐状态序列`encoder_hiddens`（大小为N \\* L \\* H）；\n",
    "    - 类似于EncoderRNN，使用nn.Embedding和nn.GRUCell，并且使用了之前定义的注意力模块Attention；\n",
    "    - 在每个时间片，依次执行\n",
    "        - 将`input`通过Embedding映射为词向量；\n",
    "        - 将词向量与前一个时间片的隐状态concat起来，经过一个线性变换`h2q`后，对编码器隐状态序列做attention，得到`attn_output`；\n",
    "        - 将`input`与`hidden`输入到GRUCell中更新隐状态`hidden`；\n",
    "        - 将词向量与attention的结果concat起来作为GRUCell的输入向量，与`hidden`一起输入到GRUCell中，得到当前时间片的隐状态`hidden`；\n",
    "        - 将`hidden`与attention的结果concat起来，经过线性变换`h2o`和LogSoftmax得到输出`output`；\n",
    "    - 输出包括当前时间片的隐状态`hidden`（大小为N \\* H）和输出`output`（大小为N \\* V，其中V为目标语言的词表大小）\n",
    "- Seq2Seq类包含了一个编码器模块和一个解码器模块，训练时解码器端使用teacher forcing（使用标准答案的单词编号作为输入，而非模型的预测结果），预测时解码器端使用贪心的解码策略（每个时间片预测概率最大的单词作为下一个时间片的输入）。\n",
    "\n",
    "为简便起见，本次实现中Seq2Seq中的所有数据的batch size都可看作1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33c0c343-ac6d-470f-a9e3-a09a58adc887",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10    # 最大解码长度\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        \"\"\"\n",
    "        假定编码器和解码器的hidden size相同。\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.lin = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "    \n",
    "    def forward(self, encoder_hidden_states, decoder_hidden_state):\n",
    "        \"\"\"\n",
    "        encoder_hidden_states: N * L * H\n",
    "        decoder_hidden_state: N * H\n",
    "        L为源语言长度，H为hidden size\n",
    "\n",
    "        输出attn_output（大小为N * H）\n",
    "        \"\"\"\n",
    "        dh = self.lin(decoder_hidden_state).unsqueeze(-1)     # N * H * 1\n",
    "        attn_scores = torch.bmm(encoder_hidden_states, dh)    # N * L * 1 注意力（相关性）分数\n",
    "        weights = F.softmax(attn_scores, dim=1)               # 在L维度上归一化得到权重\n",
    "        outputs = (weights * encoder_hidden_states).sum(1)    # N * H    在L维度上加权求和\n",
    "        return outputs\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRUCell(embedding_dim, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        input: N\n",
    "        hidden: N * H\n",
    "        \n",
    "        输出更新后的隐状态hidden（大小为N * H）\n",
    "        \"\"\"\n",
    "        embedding = self.embed(input)\n",
    "        hidden = self.gru(embedding, hidden)\n",
    "        return hidden\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRUCell(embedding_dim + hidden_size, hidden_size)\n",
    "        self.attn = Attention(hidden_size)\n",
    "        self.h2q = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size + hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_hiddens):\n",
    "        \"\"\"\n",
    "        input: N\n",
    "        hidden: N * H\n",
    "        encoder_hiddens: N * L * H\n",
    "        \n",
    "        输出对于下一个时间片的预测output（大小为N * V）更新后的隐状态hidden（大小为N * H）\n",
    "        \"\"\"\n",
    "        embedding = self.embed(input)\n",
    "        attn_query = self.h2q(torch.cat((embedding, hidden), dim=-1))\n",
    "        attn_output = self.attn(encoder_hiddens, attn_query)\n",
    "        input_combined = torch.cat((embedding, attn_output), dim=-1)\n",
    "        hidden = self.gru(input_combined, hidden)\n",
    "        output = self.h2o(torch.cat((attn_output, hidden), dim=-1))\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, embedding_dim, hidden_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = EncoderRNN(len(src_vocab), embedding_dim, hidden_size)\n",
    "        self.decoder = DecoderRNN(len(tgt_vocab), embedding_dim, hidden_size)\n",
    "        \n",
    "        self.register_buffer(\"_hidden\", torch.zeros(1, hidden_size))\n",
    "        self.register_buffer(\"_tgt_bos\", torch.full((1, ), tgt_vocab.index(\"[BOS]\")))\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \"\"\"\n",
    "        初始化编码器端隐状态为全0向量（大小为1 * H）\n",
    "        \"\"\"\n",
    "        return torch.zeros_like(self._hidden)\n",
    "    \n",
    "    def init_tgt_bos(self):\n",
    "        \"\"\"\n",
    "        预测时，初始化解码器端输入为[BOS]（大小为1）\n",
    "        \"\"\"\n",
    "        return torch.full_like(self._tgt_bos, self.tgt_vocab.index(\"[BOS]\"))\n",
    "    \n",
    "    def forward_encoder(self, src):\n",
    "        \"\"\"\n",
    "        src: N * L\n",
    "        编码器前向传播，输出最终隐状态hidden (N * H)和隐状态序列encoder_hiddens (N * L * H)\n",
    "        \"\"\"\n",
    "        _, Ls = src.size()\n",
    "        hidden = self.init_hidden()\n",
    "        encoder_hiddens = []\n",
    "        # 编码器端每个时间片，取出输入单词的下标，与上一个时间片的隐状态一起送入encoder，得到更新后的隐状态，存入enocder_hiddens\n",
    "        for i in range(Ls):\n",
    "            input = src[:, i]\n",
    "            hidden = self.encoder(input, hidden)\n",
    "            encoder_hiddens.append(hidden)\n",
    "        encoder_hiddens = torch.stack(encoder_hiddens, dim=1)\n",
    "        return hidden, encoder_hiddens\n",
    "    \n",
    "    def forward_decoder(self, tgt, hidden, encoder_hiddens):\n",
    "        \"\"\"\n",
    "        tgt: N\n",
    "        hidden: N * H\n",
    "        encoder_hiddens: N * L * H\n",
    "        \n",
    "        解码器前向传播，用于训练，使用teacher forcing，输出预测结果outputs，大小为N * L * V，其中V为目标语言词表大小\n",
    "        \"\"\"\n",
    "        _, Lt = tgt.size()\n",
    "        outputs = []\n",
    "        for i in range(Lt):\n",
    "            input = tgt[:, i]    # teacher forcing, 使用标准答案的单词作为输入，而非模型预测值\n",
    "            output, hidden = self.decoder(input, hidden, encoder_hiddens)\n",
    "            outputs.append(output)\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "            src: 1 * Ls\n",
    "            tgt: 1 * Lt\n",
    "            \n",
    "            训练时的前向传播\n",
    "        \"\"\"\n",
    "        hidden, encoder_hiddens = self.forward_encoder(src)\n",
    "        outputs = self.forward_decoder(tgt, hidden, encoder_hiddens)\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, src):\n",
    "        \"\"\"\n",
    "            src: 1 * Ls\n",
    "            \n",
    "            用于预测，解码器端初始输入为[BOS]，之后每个位置的输入为上个时间片预测概率最大的单词\n",
    "            当解码长度超过MAX_LEN或预测出了[EOS]时解码终止\n",
    "            输出预测的单词编号序列，大小为1 * L，L为预测长度\n",
    "        \"\"\"\n",
    "        hidden, encoder_hiddens = self.forward_encoder(src)\n",
    "        input = self.init_tgt_bos()\n",
    "        preds = [input]\n",
    "        while len(preds) < MAX_LEN:\n",
    "            output, hidden = self.decoder(input, hidden, encoder_hiddens)\n",
    "            input = output.argmax(-1)\n",
    "            preds.append(input)\n",
    "            if input == self.tgt_vocab.index(\"[EOS]\"):\n",
    "                break\n",
    "        preds = torch.stack(preds, dim=-1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a30b61-826a-4329-8643-aca4955f1354",
   "metadata": {},
   "source": [
    "下面这段代码创建了对应的DataLoader，从中加载得到的每一个batch由两个list构成，每个list包含了batch size个tensor，其中\n",
    "- 第一个list中的tensor对应于源语言的单词编号序列；\n",
    "- 第一个list中的tensor对应于目标语言的单词编号序列；\n",
    "- 每个tensor大小为即为序列的长度L，其中第一个元素对应于\\[BOS\\]，最后一个元素对应于\\[EOS\\]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e11f5fab-9c30-4c94-a1a7-e2d96f9abae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data_list):\n",
    "    src = [torch.tensor(_[0]) for _ in data_list]\n",
    "    tgt = [torch.tensor(_[1]) for _ in data_list]\n",
    "    return src, tgt\n",
    "\n",
    "batch_size = 16\n",
    "trainloader = torch.utils.data.DataLoader([\n",
    "    (fr_vocab.encode(fr), en_vocab.encode(en)) for fr, en in zip(train_fr, train_en)\n",
    "], batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "validloader = torch.utils.data.DataLoader([\n",
    "    (fr_vocab.encode(fr), en_vocab.encode(en)) for fr, en in zip(valid_fr, valid_en)\n",
    "], batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "testloader = torch.utils.data.DataLoader([\n",
    "    (fr_vocab.encode(fr), en_vocab.encode(en)) for fr, en in zip(test_fr, test_en)\n",
    "], batch_size=batch_size, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "356cb1cc-e298-459f-b3bf-7c77b5c0c2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([   0,   35,   14, 3643,    9,    1]), tensor([   0,    6,   43,   71,  340,   80,   52, 3968,    9,    1]), tensor([  0,   6,  24,  43, 207,  71, 156,   9,   1]), tensor([  0,  23, 124, 126,  71,  41, 408,   9,   1]), tensor([  0,   6,  46,  31, 198,  71, 574,   9,   1]), tensor([   0,  379,  106,   14,  195, 1111,    9,    1]), tensor([  0, 751,  80, 169, 379,   9,   1]), tensor([   0,   97, 1624, 1262,    5,    1]), tensor([   0,  828,   13, 2126,    9,    1]), tensor([  0,  35,  14, 474, 425,   9,   1]), tensor([  0,   6, 244, 139, 390,   9,   1]), tensor([   0, 1938,    5,    1]), tensor([   0,  145,   80, 4779,    9,    1]), tensor([  0,  23,  41, 320, 816,   9,   1]), tensor([   0,   23,  101, 1011, 1595,    9,    1]), tensor([   0, 5393,   96,  304,    5,    1])]\n",
      "[tensor([   0,   51,   98, 2332,    5,    1]), tensor([   0,    6,    7, 3138,    5,    1]), tensor([  0,   6, 126, 141,   5,   1]), tensor([  0,  27,  28, 452,   5,   1]), tensor([  0,   6, 171,  51, 467,   5,   1]), tensor([   0,   51, 1710, 1711,    5,    1]), tensor([  0, 224, 190,  22,  93,   5,   1]), tensor([   0,   88, 1224,    5,    1]), tensor([  0, 103,  13, 833,   5,   1]), tensor([  0,  93,  98,  31, 352,   5,   1]), tensor([  0,   6, 953, 265,   5,   1]), tensor([  0, 232,  51,  35,   1]), tensor([   0,  134, 2862,    5,    1]), tensor([  0,  70,  52, 644,   5,   1]), tensor([  0,  70, 579, 321, 609,   5,   1]), tensor([  0, 176,  54, 996,   5,   1])]\n"
     ]
    }
   ],
   "source": [
    "for x, y in trainloader:\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220766c-eaae-4802-9cb3-b885f0efc3cc",
   "metadata": {},
   "source": [
    "训练和预测的代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a405ae34-e755-4f92-b10f-4c05027621de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")  # 训练过程使用CPU耗时约15分钟，使用RTX2080Ti耗时约6分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a98cd0d4-025e-4337-9b84-9469b677612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, criterion, loader):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for src, tgt in tqdm(loader):\n",
    "        B = len(src)\n",
    "        loss = 0.0\n",
    "        for _ in range(B):\n",
    "            _src = src[_].unsqueeze(0).to(device)     # 1 * L\n",
    "            _tgt = tgt[_].unsqueeze(0).to(device)     # 1 * L\n",
    "            outputs = model(_src, _tgt)     # 1 * L * V\n",
    "            \n",
    "            # decoder端，每个位置的输出预测的是下一个位置的单词，因此需要错一位计算loss\n",
    "            loss += criterion(outputs[:,:-1,:].squeeze(0), _tgt[:,1:].squeeze(0))\n",
    "        \n",
    "        loss /= B\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)     # 裁剪梯度，将梯度范数裁剪为1，使训练更稳定\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def test_loop(model, loader, tgt_vocab):\n",
    "    model.eval()\n",
    "    bleu = BLEU(force=True)\n",
    "    hypotheses, references = [], []\n",
    "    for src, tgt in tqdm(loader):\n",
    "        B = len(src)\n",
    "        for _ in range(B):\n",
    "            _src = src[_].unsqueeze(0).to(device)     # 1 * L\n",
    "            with torch.no_grad():\n",
    "                outputs = model.predict(_src)         # 1 * L\n",
    "            \n",
    "            # 保留预测结果，使用词表vocab解码成文本，并删去BOS与EOS\n",
    "            ref = \" \".join(tgt_vocab.decode(tgt[_].tolist(), strip_bos_and_eos=True))\n",
    "            hypo = \" \".join(tgt_vocab.decode(outputs[0].cpu().tolist(), strip_bos_and_eos=True))\n",
    "            references.append(ref)    # 标准答案\n",
    "            hypotheses.append(hypo)   # 预测结果\n",
    "    \n",
    "    score = bleu.corpus_score(hypotheses, [references]).score      # 计算BLEU分数\n",
    "    return hypotheses, references, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bea53359-2d6f-49f6-9d9c-a786a962b671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:41<00:00,  9.90it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:03<00:00, 38.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 2.4497014626264573, valid bleu = 17.31416881798411\n",
      "we lost .\n",
      "we were busy .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:40<00:00,  9.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:03<00:00, 38.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 1.3565254182219506, valid bleu = 30.020035743080683\n",
      "we lost .\n",
      "we lost .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:38<00:00, 10.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:03<00:00, 39.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss = 0.8614444798231125, valid bleu = 36.770424787633395\n",
      "we lost .\n",
      "we lost .\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = Seq2Seq(fr_vocab, en_vocab, embedding_dim=256, hidden_size=256)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "best_score = 0.0\n",
    "for _ in range(3):\n",
    "    loss = train_loop(model, optimizer, criterion, trainloader)\n",
    "    hypotheses, references, bleu_score = test_loop(model, validloader, en_vocab)\n",
    "    # 保存验证集上bleu最高的checkpoint\n",
    "    if bleu_score > best_score:\n",
    "        torch.save(model.state_dict(), \"model_best.pt\")\n",
    "        best_score = bleu_score\n",
    "    print(f\"Epoch {_}: loss = {loss}, valid bleu = {bleu_score}\")\n",
    "    print(references[0])\n",
    "    print(hypotheses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da250a65-5530-48c9-8d6a-d1497d449658",
   "metadata": {},
   "source": [
    "加载验证集上bleu最高的模型，在测试集上进行评测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa9f1ca6-a001-43fb-a653-bc8deb46ccb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:03<00:00, 35.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test bleu = 36.195883531271846\n",
      "get a job .\n",
      "get a job .\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_best.pt\"))\n",
    "hypotheses, references, bleu_score = test_loop(model, testloader, en_vocab)\n",
    "print(f\"Test bleu = {bleu_score}\")\n",
    "print(references[0])\n",
    "print(hypotheses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3270e-78dd-4999-8528-67294f41af85",
   "metadata": {},
   "source": [
    "**作业要求：** 理解关于Seq2Seq模型部分的代码，**分别**进行以下两处修改并进行实验：\n",
    "1. 上面代码中，解码器共同利用hidden与attn_output得到输出，**请你修改为只使用attention向量预测输出的方案，并进行实验**；\n",
    "2. 上面代码实现了训练时使用teacher forcing的方案，**请你在模型中加入一个可以手动调整的参数teacher_forcing_ratio，并修改forward_decoder方法，使得在训练时以概率p=teacher_forcing_ratio使用teacher forcing，以1-p的概率不使用teacher forcing（即使用模型预测的输出作为下一个时间片的输入），并在这个参数为0, 0.5, 1时分别进行实验，并比较实验结果**。\n",
    "\n",
    "*注意训练时如果不使用teacher forcing，应保持预测长度不超过真实答案长度，以便于计算loss*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45952a4f-0f2e-4b7e-9a92-1f0379f76a76",
   "metadata": {},
   "source": [
    "## 3. 附加题\n",
    "\n",
    "**可选** 实现beam search，并在测试集上计算BLEU分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7895f4-c978-4b50-8892-cef6a7fc5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def beam_search(model, src, beam_size=5):\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "model.eval()\n",
    "bleu = BLEU(force=True)\n",
    "hypotheses, references = [], []\n",
    "for src, tgt in tqdm(testloader):\n",
    "    B = len(src)\n",
    "    for _ in range(B):\n",
    "        # TODO\n",
    "        raise NotImplementedError\n",
    "\n",
    "score = bleu.corpus_score(hypotheses, [references]).score      # 计算BLEU分数\n",
    "print(f\"Beam search (beam size = 5): BLEU = {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
